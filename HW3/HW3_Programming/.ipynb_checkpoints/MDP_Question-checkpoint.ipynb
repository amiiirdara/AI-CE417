{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction\n",
    "\n",
    "In this section, we will introduce the concepts of MDP, Q-values, and V-values. These concepts are fundamental to the field of AI and machine learning, as they are used to model **decision-making problems** in various domains such as \"robotics\", \"finance\", and \"healthcare\".\n",
    "\n",
    "MDP stands for Markov Decision Process. It is a mathematical framework for modeling decision-making problems in which the outcomes are partly random and partly under the control of a decision-maker. MDPs are defined by a set of states, a set of actions, a reward function, and a transition function. The goal is to find a policy that maximizes the expected cumulative reward over time.\n",
    "\n",
    "Q-values and V-values are two important concepts in the context of MDPs. A Q-value represents the expected cumulative reward of taking a particular action in a particular state and following a specific policy thereafter. A V-value represents the expected cumulative reward of being in a particular state and following a specific policy thereafter. These values are used to evaluate and improve the policy of an agent in an MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: The Basics of MDPs\n",
    "\n",
    "In this section, we will explain the basic components of an MDP.\n",
    "\n",
    "An MDP is defined by \"a set of states\", \"a set of actions\", \"a reward function\", and \"a transition function\". The state space is the set of all possible states that the agent can be in. The action space is the set of all possible actions that the agent can take. The reward function defines the reward the agent receives for each action taken in a particular state. The transition function defines the probability of moving from one state to another state after taking a particular action.\n",
    "\n",
    "To illustrate these concepts, let's consider an example of a **robot that needs to navigate through a maze**. The robot can be in one of several states, such as at the start of the maze, at a junction in the maze, or at the end of the maze. This robot takes an action. With Probability of **0.8** It goes in that desired direction but with probability of **0.2** It goes in the perpendicular direction (0.1, 0.1 for each)!\n",
    "\n",
    "In an MDP, the agent interacts with the environment by selecting actions based on its current state and the expected future reward. The goal of the agent is to find a policy that maximizes the expected cumulative reward over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**\n",
    "\n",
    "1. What are the state space, action space, reward function, and transition function of the robot in the maze example? Explain why you think each of these components is important for the robot to navigate through the maze.\n",
    "\n",
    "2. Is our environment stochastic or deterministic? Why?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define The MDP**:\n",
    "\n",
    "Based on your choice of rewards and transitions and the state space, define the MDP for the robot in the maze example. You can complete the following code to define the MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Maze is:\n",
      " [[2 0 0 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 1 1 1 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "maze = np.array([[2, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 0, 1],\n",
    "                 [0, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 1, 3]])\n",
    "\n",
    "print(\"Our Maze is:\\n\", maze)\n",
    "\n",
    "up = '↑'\n",
    "left = '←'\n",
    "right = '→'\n",
    "down = '↓'\n",
    "\n",
    "movement_actions = [up, down, left, right]\n",
    "num_actions = len(movement_actions)\n",
    "\n",
    "rewards = np.zeros_like(maze, dtype=float)\n",
    "rewards[maze == 3] = 1\n",
    "rewards[maze == 0] = -1\n",
    "rewards[maze == 1] = -0.1\n",
    "\n",
    "transition_probs = np.zeros((maze.size, num_actions, maze.size), dtype=float)\n",
    "movement_vectors = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "state_index = 0\n",
    "while state_index < maze.size:\n",
    "    action_index = 0\n",
    "    while action_index < num_actions:\n",
    "        for intended_next_state in [tuple(np.array(state_index) + np.array(movement_vectors[action_index]))]:\n",
    "            intended_next_state = tuple(np.maximum(np.minimum(intended_next_state, [maze.shape[0] - 1, maze.shape[1] - 1]), [0, 0]))\n",
    "            transition_probs[state_index, action_index, np.ravel_multi_index(intended_next_state, maze.shape)] += 0.8\n",
    "\n",
    "        for left_state in [tuple(np.array(state_index) + np.array(movement_vectors[(action_index - 1) % num_actions]))]:\n",
    "            left_state = tuple(np.maximum(np.minimum(left_state, [maze.shape[0] - 1, maze.shape[1] - 1]), [0, 0]))\n",
    "            transition_probs[state_index, action_index, np.ravel_multi_index(left_state, maze.shape)] += 0.1\n",
    "\n",
    "        for right_state in [tuple(np.array(state_index) + np.array(movement_vectors[(action_index + 1) % num_actions]))]:\n",
    "            right_state = tuple(np.maximum(np.minimum(right_state, [maze.shape[0] - 1, maze.shape[1] - 1]), [0, 0]))\n",
    "            transition_probs[state_index, action_index, np.ravel_multi_index(right_state, maze.shape)] += 0.1\n",
    "\n",
    "        action_index += 1\n",
    "    state_index += 1\n",
    "\n",
    "discount = 0.9\n",
    "values = np.zeros_like(maze, dtype=float)\n",
    "q_values = np.zeros((maze.size, num_actions), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Computing V-values\n",
    "\n",
    "In this section, we will explain how to compute V-values for an MDP using the Bellman equation.\n",
    "\n",
    "The Bellman equation is a recursive equation that expresses the value of a state in terms of the values of its successor states. It is defined as:\n",
    "\n",
    "$$V(s) = R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n",
    "\n",
    "where V(s) is the value of state s, R(s) is the reward for being in state s, γ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, and max_a is the maximum over all possible actions a.\n",
    "\n",
    "To compute the V-values for an MDP, we start with an initial estimate of the V-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n",
    "\n",
    "$$V(s) \\leftarrow R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n",
    "\n",
    "We can use dynamic programming algorithms such as value iteration or policy iteration to compute the V-values.\n",
    "\n",
    "We can use the Bellman equation to compute the V-values for each state in the maze. The V-values represent the expected cumulative reward that the robot can obtain if it starts from that state and follows an optimal policy thereafter. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.80461243 8.59314098 8.80973439 9.68973439 9.99973439]\n",
      " [9.99973439 9.99973439 9.99973439 9.99973439 9.99973439]\n",
      " [9.99973439 9.99973439 9.99973439 9.99973439 9.99973439]\n",
      " [9.99973439 9.99973439 9.99973439 9.99973439 9.99973439]]\n"
     ]
    }
   ],
   "source": [
    "values = np.zeros((maze.shape[0], maze.shape[1]))\n",
    "flatten_values = values.flatten()\n",
    "\n",
    "iteration = 0\n",
    "while iteration < 100:\n",
    "    state_index = 0\n",
    "    while state_index < len(states):\n",
    "        q_values = np.zeros(num_actions)\n",
    "        action_index = 0\n",
    "        while action_index < num_actions:\n",
    "            temp = 0\n",
    "            next_state_index = 0\n",
    "            while next_state_index < len(states):\n",
    "                temp += transition_probs[state_index, action_index, next_state_index] * (rewards.flatten()[next_state_index] + discount * values.flatten()[next_state_index])\n",
    "                next_state_index += 1\n",
    "            q_values[action_index] = temp\n",
    "            action_index += 1\n",
    "        max_q_value = np.max(q_values)\n",
    "        flatten_values[state_index] = max_q_value\n",
    "        state_index += 1\n",
    "    values = flatten_values.reshape(maze.shape)\n",
    "    iteration += 1\n",
    "\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Computing Q-values\n",
    "\n",
    "In this section, we will explain how to compute Q-values for an MDP using the Bellman equation.\n",
    "\n",
    "The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. The Q-values can be computed using the Bellman equation as follows:\n",
    "\n",
    "$$Q(s, a) = R(s, a) + \\gamma * \\sum_{s'} (P(s, a, s') * \\max_{a'} (Q(s', a')))$$\n",
    "\n",
    "where Q(s, a) is the Q-value of state-action pair (s, a), R(s, a) is the reward for taking action a in state s, γ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, max_a' is the maximum over all possible actions a' in state s', and sum_s' is the sum over all possible successor states s' of state s.\n",
    "\n",
    "To compute the Q-values for an MDP, we start with an initial estimate of the Q-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n",
    "\n",
    "$$Q(s, a) \\leftarrow R(s, a) + \\gamma * \\sum_{s'} (P(s, a, s') * \\max_{a'} (Q(s', a')))$$\n",
    "\n",
    "We can use dynamic programming algorithms such as Q-learning or SARSA to compute the Q-values.\n",
    "\n",
    "\n",
    "We can use the Q-learning algorithm to compute the Q-values for each state-action pair in the maze. The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.09267974 7.804639   7.09267974 6.79189174]\n",
      " [7.07701369 7.87316754 8.08976095 8.59316754]\n",
      " [8.80976095 8.80976095 8.08976095 8.08976095]\n",
      " [8.28976095 8.80976095 9.00976095 9.68976095]\n",
      " [9.99976095 9.88976095 9.11976095 9.88976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]\n",
      " [9.99976095 9.99976095 9.99976095 9.99976095]]\n"
     ]
    }
   ],
   "source": [
    "q_values = np.zeros((maze.size, num_actions))\n",
    "iterator = 0\n",
    "while iterator < 10:\n",
    "    state = 0\n",
    "    while state < len(states):\n",
    "        action = 0\n",
    "        while action < num_actions:\n",
    "            temp_val = 0\n",
    "            state_prime = 0\n",
    "            while state_prime < len(states):\n",
    "                temp_val += transition_probs[state, action, state_prime] * \\\n",
    "                            (rewards.flatten()[state_prime] + discount * values.flatten()[state_prime])\n",
    "                state_prime += 1\n",
    "            q_values[state, action] = temp_val\n",
    "            action += 1\n",
    "        state += 1\n",
    "    iterator += 1\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Visualizing the Optimal Policy\n",
    "\n",
    "Now that we have computed the Q-values, we can use them to find the optimal policy, which is the sequence of actions that the robot should take in each state to maximize its expected reward. We can visualize the optimal policy as arrows in a grid, where each arrow corresponds to the action with the highest Q-value in the corresponding state. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['↑', '↓', '←', '→']\n",
      "[[1 3 0 3 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "policy = np.zeros(maze.shape)\n",
    "policy_flatten = np.argmax(q_values, axis=1)\n",
    "policy = policy_flatten.reshape(maze.shape)\n",
    "print(movement_actions)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASEAAADrCAYAAAAmGSB9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAIj0lEQVR4nO3dT4jcdxnH8c8WC/ZQ/0A9yLLdvQjFQ9Rk01CCZje0zcGKQtGLCkpaeslmM2lqq9WK5q+ew7IppQdpLFI96MWmFKpohDW7rHUSEE+7jVXEnoyWpjUdDyFIJNZsM7PPb377el0m/GZgnme/w5uZZcKO9Hq9AFS5qXoAYGMTIaCUCAGlRAgoJUJAKRECSr1nLQ++7bbbeuPjEwMaBWir1dWVvPbaayPXum9NERofn8jphcX+TAVsGNu3Tf7P+3wcA0qJEFBKhIBSIgSUEiGglAgBpUQIKCVCQCkRAkqJEFBKhIBSIgSUEiGglAgBpUQIKCVCQKlGRuiNN5LVlcu3DI+z3W78Mc3h0ZTzamSEfruQ3PGRy7cMh87sTLZu3pTO3j3Vo3AdmnRejYwQw6UzO5P5ueNJkhPzc+nMzhRPxDtp2nmJEDfkygt6anpnkmTH1HTm547nwP59tYNxTU08LxEq9Mef/iEXL1ysHuNdW11ZyY+ePZkDX3ssX3/8W0mSb3zziTz8yKP54TM/yPnz54sn7C/nNRgiVOTCq3/Pz7744zz36ZND+8Ien5jImeVuDh4+etX1Q0eO5cxyN2NjY0WT9Z/zGhwRKnLr6Pvy2Wc/n78svprn7juZN//xZvVI78ro6Oiarg8r5zU4a/q7Y1y/X3/3F9f1uA9vHc2rvzmf5+47mS/8/Eu5+ZabBzsY1+S86ojQgJw++Ms1Pf7PC3/K63/9Z94/8YHBDMQ7cl51GhWh3y0nmz529bVLl5Lu75OPf6Jmpnfr0be+/X8f89brb+Unn3s253+1ms88c78XdCHnVacxvxNaXUmmPpns/kry9tuXr126lDzw1WT6U8krr1RO13///YK+4/6PVo/EO3Beg9OYd0LjE8kT30kefyw527187ZH9yblzybHvJ7ffXjpe3126+K+8eeGiF/SQcF6D05gIJcn+hy/fPv7Y5dtz55Ij30tmO3UzDcp7P3hLvnz6gYzcNFI9CtfBeQ1OYz6OXbH/4eTwscv/PnQ06eyvnWeQvKCHi/MajJG1/C/aLVsme6cXFgc4zn9cuJDceuu6PBUwYNu3TWZpafGaFW/cO6ErBAg2hsZGCNgYRAgoJUJAKRECSokQUEqEgFIiBJQSIaCUCAGlRAgoJUJAKRECSokQUEqEgFIiBJQSIaCUCAGlRAgoJUJAKRECSokQUEqEgFIiBJQSIaCUCAGlRAgoJUJAKRECSokQUEqEgFIiBJQSIaCUCAGlRAgoJUJAKRECSokQUEqEgFIiBJRqbITOdrvp9XrVY/RdW/dK2rubvQarkRHqzM5k6+ZN6ezdUz1KX7V1r6S9u9lr8BoXoc7sTObnjidJTszPpTM7UzxRf7R1r6S9u9lrfTQqQld+OFPTO5MkO6amMz93PAf276sd7Aa1da+kvbvZa/2MrOUz4ZYtk73TC4sDGWR1ZSV33bk5ux98KPfcuyu77p7OqRdfygunns/TTz2ZhaWXMzY2NpDnHqS27pW0dzd79d/2bZNZWlocudZ9jXknND4xkTPL3Rw8fPSq64eOHMuZ5e5QHnrS3r2S9u5mr/XVmAglyejo6JquD4u27pW0dzd7rZ9GRQjYeEQIKCVCQCkRAkqJEFBKhIBSIgSUasw3poH2GopvTAMbkwgBpUQIKCVCQCkRAkqJEFBKhIBSIgSUEiGglAgBpUQIKCVCQCkRAkqJEFBKhIBSIgSUEiGglAgBpUQIKCVCQCkRAkqJEFBKhIBSIgSUEiGglAgBpUQIKCVCQCkRAkqJEFBKhIBSIgSUEiGglAgBpUQIKCVCQCkRAkqJEFBKhIBSjY3Q2W43vV6veoy+a+teSXt3s9dgNTJCndmZbN28KZ29e6pH6au27pW0dzd7DV7jItSZncn83PEkyYn5uXRmZ4on6o+27pW0dzd7rY9GRejKD2dqemeSZMfUdObnjufA/n21g92gtu6VtHc3e62fkbV8JtyyZbJ3emFxIIOsrqzkrjs3Z/eDD+Wee3dl193TOfXiS3nh1PN5+qkns7D0csbGxgby3IPU1r2S9u5mr/7bvm0yS0uLI9e6rzHvhMYnJnJmuZuDh49edf3QkWM5s9wdykNP2rtX0t7d7LW+GhOhJBkdHV3T9WHR1r2S9u5mr/XTqAgBG48IAaVECCglQkApEQJKiRBQSoSAUo35xjTQXkPxjWlgYxIhoJQIAaVECCglQkApEQJKiRBQSoSAUiIElBIhoJQIAaVECCglQkApEQJKiRBQSoSAUiIElBIhoJQIAaVECCglQkApEQJKiRBQSoSAUiIElBIhoJQIAaVECCglQkApEQJKiRBQSoSAUiIElBIhoJQIAaVECCglQkApEQJKiRBQqrEROtvtptfrVY/Rd23dK2nvbvYarEZGqDM7k62bN6Wzd0/1KH3V1r2S9u5mr8FrXIQ6szOZnzueJDkxP5fO7EzxRP3R1r2S9u5mr/XRqAhd+eFMTe9MkuyYms783PEc2L+vdrAb1Na9kvbuZq/1M7KWz4Rbtkz2Ti8sDmSQ1ZWV3HXn5ux+8KHcc++u7Lp7OqdefCkvnHo+Tz/1ZBaWXs7Y2NhAnnuQ2rpX0t7d7NV/27dNZmlpceRa9zXmndD4xETOLHdz8PDRq64fOnIsZ5a7Q3noSXv3Stq7m73WV2MilCSjo6Nruj4s2rpX0t7d7LV+GhUhYOMRIaCUCAGlRAgoJUJAKRECSokQUKox35gG2msovjENbEwiBJQSIaCUCAGlRAgoJUJAKRECSokQUEqEgFIiBJQSIaCUCAGlRAgoJUJAKRECSokQUEqEgFIiBJQSIaCUCAGlRAgotaa/tjEyMvK3JKuDGwdoqfFer/eha92xpggB9JuPY0ApEQJKiRBQSoSAUiIElBIhoJQIAaVECCglQkCpfwMQTS5+99nNVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy_arrows = [movement_actions[p] for p in policy_flatten]\n",
    "policy_grid = np.array(policy_arrows).reshape(maze.shape)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(np.zeros(maze.shape), cmap='Blues', extent=[0, maze.shape[1], 0, maze.shape[0]])\n",
    "for i in range(maze.shape[0]):\n",
    "    for j in range(maze.shape[1]):\n",
    "        if maze[i, j] == 2:\n",
    "            plt.text(j + 0.5, i + 0.5, 'S',\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', fontsize=18, color='red')\n",
    "        elif maze[i, j] == 3:\n",
    "            plt.text(j + 0.5, i + 0.5, 'G',\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', fontsize=18, color='green')\n",
    "        elif maze[i, j] == 1:\n",
    "            plt.text(j + 0.5, i + 0.5, 'X',\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', fontsize=18, color='black')\n",
    "        else:\n",
    "            plt.text(j + 0.5, i + 0.5, 'P',\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', fontsize=18, color='black')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(np.zeros(maze.shape), cmap='Blues', extent=[0, maze.shape[1], 0, maze.shape[0]])\n",
    "for i in range(maze.shape[0]):\n",
    "    for j in range(maze.shape[1]):\n",
    "        if policy_grid[i, j] == up:\n",
    "            plt.text(j + 0.5, i + 0.5, policy_grid[i, j],\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', fontsize=18, color='black')\n",
    "        if policy_grid[i, j] == down:\n",
    "            plt.text(j + 0.5, i + 0.5, policy_grid[i, j],\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', fontsize=18, color='blue')\n",
    "        if policy_grid[i, j] == left:\n",
    "            plt.text(j + 0.5, i + 0.5, policy_grid[i, j],\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', fontsize=18, color='orange')\n",
    "        if policy_grid[i, j] == right:\n",
    "            plt.text(j + 0.5, i + 0.5, policy_grid[i, j],\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center', fontsize=18, color='purple')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
